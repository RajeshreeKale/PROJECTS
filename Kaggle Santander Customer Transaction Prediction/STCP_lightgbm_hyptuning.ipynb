{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "## Kaggle Competition\n",
    "\n",
    "## Santander Customer Transaction Prediction\n",
    "At Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\n",
    "Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n",
    "In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.\n",
    "## Dataset\n",
    "Tha dataset is taken from https://www.kaggle.com/c/santander-customer-transaction-prediction/data\n",
    "File descriptions\n",
    "train.csv - the training set.\n",
    "test.csv - the test set.\n",
    "The test set contains some rows which are not included in scoring. sample_submission.csv - a sample submission file in the correct format.\n",
    "## Abstract\n",
    "At Santander our mission is to help people and help businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\n",
    "Our method is firstly to analyze the distribution of the dataset, the condition of balance in the dataset, average number and standard deviation of both rows and columns.\n",
    "Then we use the H2O AUTOML platform to generate the best models, and use the best model to predict the value in the test datset, and the rank is 1948 out of 3000 which was not a good score and then we started to train the model using LightGBM and predicted the target. The final step is tunning the hyperparameters of the model by adding more number of leaves, maxbin,reducing the learning rate and adding more parameters. We got even better outcome of 589 out of 3000 after tunning the hyperparameters and training the best model to predict the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'sample_submission.csv', 'Santander_3.ipynb', 'STCP_lightgbm.ipynb', 'submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"C:/Users/87421/Desktop/Kaggle/santander\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:  200000 Columns:  202\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 202 entries, ID_code to var_199\n",
      "dtypes: float64(200), int64(1), object(1)\n",
      "memory usage: 308.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('C:/Users/87421/Desktop/Kaggle/santander/train.csv')\n",
    "print('Rows: ',train_df.shape[0],'Columns: ',train_df.shape[1])\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7   ...     var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266   ...      4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338   ...      7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  14.6155   ...      2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  14.9250   ...      4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  19.2514   ...     -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    179902\n",
       "1     20098\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('C:/Users/87421/D\n",
    "                      esktop/Kaggle/santander/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.drop('ID_code',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['ID_code','target'],axis=1)\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 8,\n",
    "         'min_data_in_leaf': 42,\n",
    "         'objective': 'binary',\n",
    "         'max_depth': 16,\n",
    "         'learning_rate': 0.0123,\n",
    "         'boosting': 'gbdt',\n",
    "         'bagging_freq': 5,\n",
    "         'bagging_fraction': 0.8,\n",
    "         'feature_fraction': 0.8201,\n",
    "         'bagging_seed': 11,\n",
    "         'reg_alpha': 1.728910519108444,\n",
    "         'reg_lambda': 4.9847051755586085,\n",
    "         'random_state': 42,\n",
    "         'metric': 'auc',\n",
    "         'verbosity': -1,\n",
    "         'subsample': 0.81,\n",
    "         'min_gain_to_split': 0.01077313523861969,\n",
    "         'min_child_weight': 19.428902804238373,\n",
    "         'num_threads': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Thu Feb 28 22:51:04 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[300]\ttraining's auc: 0.827918\tvalid_1's auc: 0.814605\n",
      "[600]\ttraining's auc: 0.862802\tvalid_1's auc: 0.845745\n",
      "[900]\ttraining's auc: 0.880631\tvalid_1's auc: 0.860844\n",
      "[1200]\ttraining's auc: 0.891953\tvalid_1's auc: 0.869908\n",
      "[1500]\ttraining's auc: 0.900115\tvalid_1's auc: 0.876263\n",
      "[1800]\ttraining's auc: 0.906338\tvalid_1's auc: 0.88068\n",
      "[2100]\ttraining's auc: 0.911263\tvalid_1's auc: 0.884034\n",
      "[2400]\ttraining's auc: 0.915172\tvalid_1's auc: 0.886626\n",
      "[2700]\ttraining's auc: 0.918385\tvalid_1's auc: 0.888726\n",
      "[3000]\ttraining's auc: 0.921196\tvalid_1's auc: 0.890411\n",
      "[3300]\ttraining's auc: 0.923598\tvalid_1's auc: 0.891873\n",
      "[3600]\ttraining's auc: 0.925717\tvalid_1's auc: 0.893051\n",
      "[3900]\ttraining's auc: 0.927648\tvalid_1's auc: 0.894246\n",
      "[4200]\ttraining's auc: 0.929203\tvalid_1's auc: 0.895105\n",
      "[4500]\ttraining's auc: 0.930731\tvalid_1's auc: 0.895752\n",
      "[4800]\ttraining's auc: 0.932079\tvalid_1's auc: 0.896317\n",
      "[5100]\ttraining's auc: 0.933361\tvalid_1's auc: 0.896815\n",
      "[5400]\ttraining's auc: 0.93459\tvalid_1's auc: 0.897188\n",
      "[5700]\ttraining's auc: 0.935812\tvalid_1's auc: 0.897423\n",
      "[6000]\ttraining's auc: 0.937008\tvalid_1's auc: 0.897611\n",
      "[6300]\ttraining's auc: 0.938176\tvalid_1's auc: 0.89782\n",
      "[6600]\ttraining's auc: 0.939298\tvalid_1's auc: 0.897899\n",
      "[6900]\ttraining's auc: 0.940474\tvalid_1's auc: 0.897968\n",
      "[7200]\ttraining's auc: 0.941623\tvalid_1's auc: 0.898073\n",
      "Early stopping, best iteration is:\n",
      "[7262]\ttraining's auc: 0.941865\tvalid_1's auc: 0.898107\n",
      "Fold 1 started at Thu Feb 28 23:01:46 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[300]\ttraining's auc: 0.827751\tvalid_1's auc: 0.814418\n",
      "[600]\ttraining's auc: 0.862694\tvalid_1's auc: 0.846376\n",
      "[900]\ttraining's auc: 0.880661\tvalid_1's auc: 0.861617\n",
      "[1200]\ttraining's auc: 0.89197\tvalid_1's auc: 0.870773\n",
      "[1500]\ttraining's auc: 0.899986\tvalid_1's auc: 0.876916\n",
      "[1800]\ttraining's auc: 0.906034\tvalid_1's auc: 0.881229\n",
      "[2100]\ttraining's auc: 0.91104\tvalid_1's auc: 0.884702\n",
      "[2400]\ttraining's auc: 0.915125\tvalid_1's auc: 0.88748\n",
      "[2700]\ttraining's auc: 0.918422\tvalid_1's auc: 0.889738\n",
      "[3000]\ttraining's auc: 0.921223\tvalid_1's auc: 0.891263\n",
      "[3300]\ttraining's auc: 0.923545\tvalid_1's auc: 0.892692\n",
      "[3600]\ttraining's auc: 0.925664\tvalid_1's auc: 0.8939\n",
      "[3900]\ttraining's auc: 0.927489\tvalid_1's auc: 0.894689\n",
      "[4200]\ttraining's auc: 0.929129\tvalid_1's auc: 0.895636\n",
      "[4500]\ttraining's auc: 0.930505\tvalid_1's auc: 0.896254\n",
      "[4800]\ttraining's auc: 0.931932\tvalid_1's auc: 0.896753\n",
      "[5100]\ttraining's auc: 0.933245\tvalid_1's auc: 0.897245\n",
      "[5400]\ttraining's auc: 0.934434\tvalid_1's auc: 0.897676\n",
      "[5700]\ttraining's auc: 0.935576\tvalid_1's auc: 0.898036\n",
      "[6000]\ttraining's auc: 0.93674\tvalid_1's auc: 0.898167\n",
      "[6300]\ttraining's auc: 0.937896\tvalid_1's auc: 0.898315\n",
      "[6600]\ttraining's auc: 0.939002\tvalid_1's auc: 0.898441\n",
      "[6900]\ttraining's auc: 0.940133\tvalid_1's auc: 0.898543\n",
      "[7200]\ttraining's auc: 0.941265\tvalid_1's auc: 0.898681\n",
      "Early stopping, best iteration is:\n",
      "[7270]\ttraining's auc: 0.941529\tvalid_1's auc: 0.898721\n",
      "Fold 2 started at Thu Feb 28 23:09:25 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[300]\ttraining's auc: 0.827051\tvalid_1's auc: 0.814447\n",
      "[600]\ttraining's auc: 0.862015\tvalid_1's auc: 0.846472\n",
      "[900]\ttraining's auc: 0.879556\tvalid_1's auc: 0.862552\n",
      "[1200]\ttraining's auc: 0.890617\tvalid_1's auc: 0.872344\n",
      "[1500]\ttraining's auc: 0.898819\tvalid_1's auc: 0.879328\n",
      "[1800]\ttraining's auc: 0.905134\tvalid_1's auc: 0.88467\n",
      "[2100]\ttraining's auc: 0.909774\tvalid_1's auc: 0.888424\n",
      "[2400]\ttraining's auc: 0.913715\tvalid_1's auc: 0.891313\n",
      "[2700]\ttraining's auc: 0.91699\tvalid_1's auc: 0.893762\n",
      "[3000]\ttraining's auc: 0.919726\tvalid_1's auc: 0.895966\n",
      "[3300]\ttraining's auc: 0.922112\tvalid_1's auc: 0.897568\n",
      "[3600]\ttraining's auc: 0.924303\tvalid_1's auc: 0.898953\n",
      "[3900]\ttraining's auc: 0.926179\tvalid_1's auc: 0.899962\n",
      "[4200]\ttraining's auc: 0.927827\tvalid_1's auc: 0.900843\n",
      "[4500]\ttraining's auc: 0.929295\tvalid_1's auc: 0.90161\n",
      "[4800]\ttraining's auc: 0.930666\tvalid_1's auc: 0.902228\n",
      "[5100]\ttraining's auc: 0.932031\tvalid_1's auc: 0.902531\n",
      "[5400]\ttraining's auc: 0.933261\tvalid_1's auc: 0.902963\n",
      "[5700]\ttraining's auc: 0.934496\tvalid_1's auc: 0.90331\n",
      "[6000]\ttraining's auc: 0.935635\tvalid_1's auc: 0.90353\n",
      "[6300]\ttraining's auc: 0.936773\tvalid_1's auc: 0.903782\n",
      "[6600]\ttraining's auc: 0.937967\tvalid_1's auc: 0.903926\n",
      "[6900]\ttraining's auc: 0.939166\tvalid_1's auc: 0.90405\n",
      "[7200]\ttraining's auc: 0.940283\tvalid_1's auc: 0.904113\n",
      "[7500]\ttraining's auc: 0.941393\tvalid_1's auc: 0.904212\n",
      "[7800]\ttraining's auc: 0.942585\tvalid_1's auc: 0.904267\n",
      "[8100]\ttraining's auc: 0.943663\tvalid_1's auc: 0.9043\n",
      "[8400]\ttraining's auc: 0.94479\tvalid_1's auc: 0.904334\n",
      "Early stopping, best iteration is:\n",
      "[8289]\ttraining's auc: 0.944369\tvalid_1's auc: 0.904366\n",
      "Fold 3 started at Thu Feb 28 23:17:27 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[300]\ttraining's auc: 0.827841\tvalid_1's auc: 0.807053\n",
      "[600]\ttraining's auc: 0.86207\tvalid_1's auc: 0.840497\n",
      "[900]\ttraining's auc: 0.880144\tvalid_1's auc: 0.856355\n",
      "[1200]\ttraining's auc: 0.891374\tvalid_1's auc: 0.866072\n",
      "[1500]\ttraining's auc: 0.900156\tvalid_1's auc: 0.873288\n",
      "[1800]\ttraining's auc: 0.906403\tvalid_1's auc: 0.87834\n",
      "[2100]\ttraining's auc: 0.911156\tvalid_1's auc: 0.881947\n",
      "[2400]\ttraining's auc: 0.915244\tvalid_1's auc: 0.884734\n",
      "[2700]\ttraining's auc: 0.918671\tvalid_1's auc: 0.887051\n",
      "[3000]\ttraining's auc: 0.921501\tvalid_1's auc: 0.888721\n",
      "[3300]\ttraining's auc: 0.92391\tvalid_1's auc: 0.890267\n",
      "[3600]\ttraining's auc: 0.926001\tvalid_1's auc: 0.891602\n",
      "[3900]\ttraining's auc: 0.927806\tvalid_1's auc: 0.892728\n",
      "[4200]\ttraining's auc: 0.929385\tvalid_1's auc: 0.8936\n",
      "[4500]\ttraining's auc: 0.930864\tvalid_1's auc: 0.89434\n",
      "[4800]\ttraining's auc: 0.93224\tvalid_1's auc: 0.894984\n",
      "[5100]\ttraining's auc: 0.933493\tvalid_1's auc: 0.89557\n",
      "[5400]\ttraining's auc: 0.934708\tvalid_1's auc: 0.895962\n",
      "[5700]\ttraining's auc: 0.935878\tvalid_1's auc: 0.896372\n",
      "[6000]\ttraining's auc: 0.937057\tvalid_1's auc: 0.89667\n",
      "[6300]\ttraining's auc: 0.93822\tvalid_1's auc: 0.896848\n",
      "[6600]\ttraining's auc: 0.939343\tvalid_1's auc: 0.896974\n",
      "[6900]\ttraining's auc: 0.940541\tvalid_1's auc: 0.897128\n",
      "[7200]\ttraining's auc: 0.941677\tvalid_1's auc: 0.897262\n",
      "[7500]\ttraining's auc: 0.942832\tvalid_1's auc: 0.897298\n",
      "[7800]\ttraining's auc: 0.94396\tvalid_1's auc: 0.897443\n",
      "[8100]\ttraining's auc: 0.945075\tvalid_1's auc: 0.897549\n",
      "[8400]\ttraining's auc: 0.946161\tvalid_1's auc: 0.897663\n",
      "Early stopping, best iteration is:\n",
      "[8446]\ttraining's auc: 0.946319\tvalid_1's auc: 0.897674\n",
      "Fold 4 started at Thu Feb 28 23:25:41 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[300]\ttraining's auc: 0.829989\tvalid_1's auc: 0.811794\n",
      "[600]\ttraining's auc: 0.864709\tvalid_1's auc: 0.840536\n",
      "[900]\ttraining's auc: 0.882118\tvalid_1's auc: 0.854439\n",
      "[1200]\ttraining's auc: 0.893194\tvalid_1's auc: 0.863714\n",
      "[1500]\ttraining's auc: 0.901151\tvalid_1's auc: 0.869946\n",
      "[1800]\ttraining's auc: 0.906977\tvalid_1's auc: 0.874971\n",
      "[2100]\ttraining's auc: 0.911673\tvalid_1's auc: 0.878799\n",
      "[2400]\ttraining's auc: 0.915598\tvalid_1's auc: 0.881841\n",
      "[2700]\ttraining's auc: 0.918896\tvalid_1's auc: 0.884324\n",
      "[3000]\ttraining's auc: 0.921519\tvalid_1's auc: 0.88601\n",
      "[3300]\ttraining's auc: 0.923945\tvalid_1's auc: 0.887669\n",
      "[3600]\ttraining's auc: 0.926012\tvalid_1's auc: 0.88915\n",
      "[3900]\ttraining's auc: 0.927801\tvalid_1's auc: 0.890259\n",
      "[4200]\ttraining's auc: 0.929471\tvalid_1's auc: 0.89115\n",
      "[4500]\ttraining's auc: 0.930907\tvalid_1's auc: 0.891936\n",
      "[4800]\ttraining's auc: 0.932269\tvalid_1's auc: 0.892578\n",
      "[5100]\ttraining's auc: 0.933583\tvalid_1's auc: 0.893007\n",
      "[5400]\ttraining's auc: 0.934782\tvalid_1's auc: 0.893408\n",
      "[5700]\ttraining's auc: 0.936039\tvalid_1's auc: 0.893808\n",
      "[6000]\ttraining's auc: 0.937277\tvalid_1's auc: 0.894164\n",
      "[6300]\ttraining's auc: 0.938437\tvalid_1's auc: 0.894302\n",
      "[6600]\ttraining's auc: 0.939574\tvalid_1's auc: 0.894535\n",
      "[6900]\ttraining's auc: 0.940683\tvalid_1's auc: 0.894651\n",
      "[7200]\ttraining's auc: 0.94187\tvalid_1's auc: 0.894709\n",
      "[7500]\ttraining's auc: 0.942978\tvalid_1's auc: 0.894746\n",
      "[7800]\ttraining's auc: 0.9441\tvalid_1's auc: 0.894834\n",
      "[8100]\ttraining's auc: 0.945225\tvalid_1's auc: 0.894851\n",
      "Early stopping, best iteration is:\n",
      "[7952]\ttraining's auc: 0.944654\tvalid_1's auc: 0.894875\n"
     ]
    }
   ],
   "source": [
    "prediction = np.zeros(len(X_test))\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        \n",
    "    model = lgb.train(params,train_data,num_boost_round=20000,\n",
    "                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 200)\n",
    "            \n",
    "    #y_pred_valid = model.predict(X_valid)\n",
    "    prediction += model.predict(X_test, num_iteration=model.best_iteration)/5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LightGBM model gave the accuracy of 89.48% using the Santander Customer Transcation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from catboost import CatBoostClassifier,Pool\\nprediction1 = np.zeros(len(X_test))\\nm = CatBoostClassifier(loss_function=\"Logloss\",eval_metric=\"AUC\",\\n                       boosting_type = \\'Ordered\\')\\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\\n    print(\\'Fold\\', fold_n, \\'started at\\', time.ctime())\\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\\n\\n    train_data = Pool(X_train, label=y_train)\\n    valid_data = Pool(X_valid, label=y_valid)\\n\\n    model1 = m.fit(train_data,eval_set=valid_data,use_best_model=True,verbose=300)\\n    \\n    prediction1 += model1.predict(X_test)/5\\n'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"from catboost import CatBoostClassifier,Pool\n",
    "prediction1 = np.zeros(len(X_test))\n",
    "m = CatBoostClassifier(loss_function=\"Logloss\",eval_metric=\"AUC\",\n",
    "                       boosting_type = 'Ordered')\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "    train_data = Pool(X_train, label=y_train)\n",
    "    valid_data = Pool(X_valid, label=y_valid)\n",
    "\n",
    "    model1 = m.fit(train_data,eval_set=valid_data,use_best_model=True,verbose=300)\n",
    "    \n",
    "    prediction1 += model1.predict(X_test)/5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create xGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri Mar  1 16:18:40 2019\n",
      "[0]\tvalidation_0-auc:0.60766\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-auc:0.871004\n",
      "[400]\tvalidation_0-auc:0.886465\n",
      "[600]\tvalidation_0-auc:0.891015\n",
      "[800]\tvalidation_0-auc:0.892374\n",
      "[1000]\tvalidation_0-auc:0.892688\n",
      "Stopping. Best iteration:\n",
      "[959]\tvalidation_0-auc:0.892841\n",
      "\n",
      "Fold 1 started at Fri Mar  1 16:32:03 2019\n",
      "[0]\tvalidation_0-auc:0.598816\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-auc:0.872368\n",
      "[400]\tvalidation_0-auc:0.887945\n",
      "[600]\tvalidation_0-auc:0.893176\n",
      "[800]\tvalidation_0-auc:0.894424\n",
      "[1000]\tvalidation_0-auc:0.894634\n",
      "[1200]\tvalidation_0-auc:0.894634\n",
      "Stopping. Best iteration:\n",
      "[1176]\tvalidation_0-auc:0.894919\n",
      "\n",
      "Fold 2 started at Fri Mar  1 16:47:28 2019\n",
      "[0]\tvalidation_0-auc:0.575435\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-auc:0.875978\n",
      "[400]\tvalidation_0-auc:0.891846\n",
      "[600]\tvalidation_0-auc:0.897075\n",
      "[800]\tvalidation_0-auc:0.898543\n",
      "[1000]\tvalidation_0-auc:0.898712\n",
      "[1200]\tvalidation_0-auc:0.898976\n",
      "Stopping. Best iteration:\n",
      "[1059]\tvalidation_0-auc:0.899095\n",
      "\n",
      "Fold 3 started at Fri Mar  1 17:01:16 2019\n",
      "[0]\tvalidation_0-auc:0.614287\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-auc:0.870106\n",
      "[400]\tvalidation_0-auc:0.886208\n",
      "[600]\tvalidation_0-auc:0.891607\n",
      "[800]\tvalidation_0-auc:0.893143\n",
      "[1000]\tvalidation_0-auc:0.893569\n",
      "Stopping. Best iteration:\n",
      "[934]\tvalidation_0-auc:0.893761\n",
      "\n",
      "Fold 4 started at Fri Mar  1 17:13:45 2019\n",
      "[0]\tvalidation_0-auc:0.619657\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-auc:0.868037\n",
      "[400]\tvalidation_0-auc:0.88494\n",
      "[600]\tvalidation_0-auc:0.890258\n",
      "[800]\tvalidation_0-auc:0.891737\n",
      "[1000]\tvalidation_0-auc:0.891894\n",
      "Stopping. Best iteration:\n",
      "[965]\tvalidation_0-auc:0.892151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = xgb.XGBClassifier(max_depth=4,n_estimators=999999, colsample_bytree=0.7,subsample = 0.7, \n",
    "                              min_child_weight = 50, eval_metric = \"auc\",gamma = 5,alpha = 0,\n",
    "                               booster = \"gbtree\",colsample_bylevel = 0.7, learning_rate=0.1,\n",
    "                              objective='binary:logistic', n_jobs=-1)\n",
    "\n",
    "prediction2 = np.zeros(len(X_test))\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    #evallist = [(valid_data, 'eval'), (train_data, 'train')]\n",
    "    model2 = mod.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose=200, eval_metric='auc',\n",
    "                        early_stopping_rounds=200)\n",
    "    \n",
    "    prediction2 += model2.predict(X_test, ntree_limit=model2.best_ntree_limit)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The XGBoost model gave the accuracy of 89.21% using the Santander Customer Transcation Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:left\"> \n",
    "    <thead> \n",
    "    <tr> <th>Number</th> <th>Model Name</th> <th>Accuray</th> </tr> </thead> <tbody> \n",
    "    <tr> \n",
    "        <tr> <th scope='row'>2</th> <td style=\"text-align:center\">XGBoosting</td>  <td>0.8921</td> </tr>\n",
    "        <tr> <th scope='row'>3</th> <td style=\"text-align:center\">CatBoosting</td>  <td>0.8910</td> </tr>\n",
    "    <tr> <th scope='row'>4</th> <td style=\"text-align:center\">LightGBM</td>  <td>0.8948</td> </tr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08114897, 0.2424331 , 0.21381503, ..., 0.003254  , 0.09354758,\n",
       "       0.07144961])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "sub[\"target\"] = prediction\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub[\"target\"] = prediction1\n",
    "#sub.to_csv(\"submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub[\"target\"] = (prediction + prediction1)/2\n",
    "#sub.to_csv(\"submission2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub[\"target\"] = prediction2\n",
    "#sub.to_csv(\"submission3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub[\"target\"] = (prediction + prediction2)/2 \n",
    "#sub.to_csv(\"submission4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub[\"target\"] = (prediction + prediction1 + prediction2)/3 \n",
    "#sub.to_csv(\"submission5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the hyperparameters of lightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM uses the leaf-wise tree growth algorithm, while many other popular tools use depth-wise tree growth. Compared with depth-wise growth, the leaf-wise algorithm can converge much faster. However, the leaf-wise growth may be over-fitting if not used with the appropriate parameters\n",
    "\n",
    "For Faster Speed\n",
    "Use bagging by setting bagging_fraction and bagging_freq\n",
    "Use feature sub-sampling by setting feature_fraction\n",
    "Use small max_bin\n",
    "Use save_binary to speed up data loading in future learning\n",
    "\n",
    "For Better Accuracy\n",
    "Use large max_bin (may be slower)\n",
    "Use small learning_rate with large num_iterations\n",
    "Use large num_leaves (may cause over-fitting)\n",
    "Use bigger training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the variables used and define the range  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier()\n",
    "params = {'num_leaves': [8, 10, 12],  #Enlarge the range of num_leaves\n",
    "         'min_data_in_leaf': [42],\n",
    "         'objective': ['binary'],\n",
    "         'max_depth': [14, 16, 18],   #Enlarge the range of max_depth(tree based hyperparameters)\n",
    "         'learning_rate': [0.0123],\n",
    "         'boosting': ['gbdt'],\n",
    "         'bagging_freq': [5],\n",
    "         'bagging_fraction': [0.8],\n",
    "         'feature_fraction': [0.8201],\n",
    "         'bagging_seed': [11],\n",
    "         'reg_alpha': [1.728910519108444],\n",
    "         'reg_lambda': [4.9847051755586085],\n",
    "         'random_state': [42],\n",
    "         'metric': ['auc'],\n",
    "         'verbosity': [-1],\n",
    "         'subsample': [0.81],\n",
    "         'min_gain_to_split': [0.01077313523861969],\n",
    "         'min_child_weight': [19.428902804238373],\n",
    "         'num_threads': [4, 8]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the grid search object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=lgbm, param_grid=params, cv = 5, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['target', 'ID_code'], inplace=False)\n",
    "y_train = train_df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the grid search model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.7min\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  8.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'num_leaves': [8, 10, 12], 'min_data_in_leaf': [42], 'objective': ['binary'], 'max_depth': [14, 16, 18], 'learning_rate': [0.0123], 'boosting': ['gbdt'], 'bagging_freq': [5], 'bagging_fraction': [0.8], 'feature_fraction': [0.8201], 'bagging_seed': [11], 'reg_alpha': [1.728910519108444], ...n_to_split': [0.01077313523861969], 'min_child_weight': [19.428902804238373], 'num_threads': [4, 8]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters we want to tune is num_leave, max_depth, and the number of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8,\n",
       " 'bagging_freq': 5,\n",
       " 'bagging_seed': 11,\n",
       " 'boosting': 'gbdt',\n",
       " 'feature_fraction': 0.8201,\n",
       " 'learning_rate': 0.0123,\n",
       " 'max_depth': 10,\n",
       " 'metric': 'auc',\n",
       " 'min_child_weight': 19.428902804238373,\n",
       " 'min_data_in_leaf': 42,\n",
       " 'min_gain_to_split': 0.01077313523861969,\n",
       " 'num_leaves': 4,\n",
       " 'num_threads': 4,\n",
       " 'objective': 'binary',\n",
       " 'random_state': 42,\n",
       " 'reg_alpha': 1.728910519108444,\n",
       " 'reg_lambda': 4.9847051755586085,\n",
       " 'subsample': 0.81,\n",
       " 'verbosity': -1}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, num_leaves = 8 from [8, 10, 12], num_threads = 4 from [4, 8], max_depth = 14 from [14, 16, 18]\n",
    "### So we can't let them become bigger. We have to reduce it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89951"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the score is pretty good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets define the lower range of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': [4, 6, 8],\n",
    "         'min_data_in_leaf': [42],\n",
    "         'objective': ['binary'],\n",
    "         'max_depth': [10, 12, 14],\n",
    "         'learning_rate': [0.0123],\n",
    "         'boosting': ['gbdt'],\n",
    "         'bagging_freq': [5],\n",
    "         'bagging_fraction': [0.8],\n",
    "         'feature_fraction': [0.8201],\n",
    "         'bagging_seed': [11],\n",
    "         'reg_alpha': [1.728910519108444],\n",
    "         'reg_lambda': [4.9847051755586085],\n",
    "         'random_state': [42],\n",
    "         'metric': ['auc'],\n",
    "         'verbosity': [-1],\n",
    "         'subsample': [0.81],\n",
    "         'min_gain_to_split': [0.01077313523861969],\n",
    "         'min_child_weight': [19.428902804238373],\n",
    "         'num_threads': [4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.2min\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\87421\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LGBMClassifier(bagging_fraction=0.55, bagging_freq=5, bagging_seed=31415,\n",
       "        boost_from_average=False, boosting='gbdt', boosting_type='gbdt',\n",
       "        class_weight=None, colsample_bytree=1.0, data_random_seed=31415,\n",
       "        drop_seed=31415, feature_fraction=0.51,\n",
       "        feature_fracti...subsample=0.81,\n",
       "        subsample_for_bin=200000, subsample_freq=0, verbose=1,\n",
       "        verbosity=-1),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'num_leaves': [4, 6, 8], 'min_data_in_leaf': [42], 'objective': ['binary'], 'max_depth': [10, 12, 14], 'learning_rate': [0.0123], 'boosting': ['gbdt'], 'bagging_freq': [5], 'bagging_fraction': [0.8], 'feature_fraction': [0.8201], 'bagging_seed': [11], 'reg_alpha': [1.728910519108444], 'r...gain_to_split': [0.01077313523861969], 'min_child_weight': [19.428902804238373], 'num_threads': [4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator=grid_search.best_estimator_, param_grid=params, cv = 5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the score becomes much lower, so we can see that num_leaves = 8 , num_threads = 4 , max_depth = 14 are the good set of hyperparamters to choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.673515"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.55, bagging_freq=5, bagging_seed=31415,\n",
       "        boost_from_average=False, boosting_type='gbdt', class_weight=None,\n",
       "        colsample_bytree=1.0, data_random_seed=31415, drop_seed=31415,\n",
       "        feature_fraction=0.51, feature_fraction_seed=31415,\n",
       "        importance_type='split', is_unbalance=True, learning_rate=0.0005,\n",
       "        max_bin=94, max_depth=14, metric='auc', min_child_samples=20,\n",
       "        min_child_weight=0.001, min_data_in_leaf=45, min_split_gain=0.0,\n",
       "        min_sum_hessian_in_leaf=0.000446, n_estimators=100, n_jobs=-1,\n",
       "        num_leaves=14, objective='binary', random_state=None,\n",
       "        reg_alpha=0.0, reg_lambda=0.0, save_binary=True, seed=31452,\n",
       "        silent=True, subsample=1.0, subsample_for_bin=200000,\n",
       "        subsample_freq=0, verbose=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we should use the parameters before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'bagging_fraction': 0.8,\n",
    "          'bagging_freq': 5,\n",
    "          'bagging_seed': 11,\n",
    "          'boosting': 'gbdt',\n",
    "          'feature_fraction': 0.8201,\n",
    "          'learning_rate': 0.0123,\n",
    "          'max_depth': 14,\n",
    "          'metric': 'auc',\n",
    "          'min_child_weight': 19.428902804238373,\n",
    "          'min_data_in_leaf': 42,\n",
    "          'min_gain_to_split': 0.01077313523861969,\n",
    "          'num_leaves': 8,\n",
    "          'num_threads': 4,\n",
    "          'objective': 'binary',\n",
    "          'random_state': 42,\n",
    "          'reg_alpha': 1.728910519108444,\n",
    "          'reg_lambda': 4.9847051755586085,\n",
    "          'subsample': 0.81,\n",
    "          'verbosity': -1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the lightGBM model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri Mar  1 01:01:16 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[300]\ttraining's auc: 0.827918\tvalid_1's auc: 0.814605\n",
      "[600]\ttraining's auc: 0.862802\tvalid_1's auc: 0.845745\n",
      "[900]\ttraining's auc: 0.880631\tvalid_1's auc: 0.860844\n",
      "[1200]\ttraining's auc: 0.891953\tvalid_1's auc: 0.869908\n",
      "[1500]\ttraining's auc: 0.900115\tvalid_1's auc: 0.876263\n",
      "[1800]\ttraining's auc: 0.906338\tvalid_1's auc: 0.88068\n",
      "[2100]\ttraining's auc: 0.911263\tvalid_1's auc: 0.884034\n",
      "[2400]\ttraining's auc: 0.915172\tvalid_1's auc: 0.886626\n",
      "[2700]\ttraining's auc: 0.918385\tvalid_1's auc: 0.888726\n",
      "[3000]\ttraining's auc: 0.921196\tvalid_1's auc: 0.890411\n",
      "[3300]\ttraining's auc: 0.923598\tvalid_1's auc: 0.891873\n",
      "[3600]\ttraining's auc: 0.925717\tvalid_1's auc: 0.893051\n",
      "[3900]\ttraining's auc: 0.927648\tvalid_1's auc: 0.894246\n",
      "[4200]\ttraining's auc: 0.929203\tvalid_1's auc: 0.895105\n",
      "[4500]\ttraining's auc: 0.930731\tvalid_1's auc: 0.895752\n",
      "[4800]\ttraining's auc: 0.932079\tvalid_1's auc: 0.896317\n",
      "[5100]\ttraining's auc: 0.933361\tvalid_1's auc: 0.896815\n",
      "[5400]\ttraining's auc: 0.93459\tvalid_1's auc: 0.897188\n",
      "[5700]\ttraining's auc: 0.935812\tvalid_1's auc: 0.897423\n",
      "[6000]\ttraining's auc: 0.937008\tvalid_1's auc: 0.897611\n",
      "[6300]\ttraining's auc: 0.938176\tvalid_1's auc: 0.89782\n",
      "[6600]\ttraining's auc: 0.939298\tvalid_1's auc: 0.897899\n",
      "[6900]\ttraining's auc: 0.940474\tvalid_1's auc: 0.897968\n",
      "[7200]\ttraining's auc: 0.941623\tvalid_1's auc: 0.898073\n",
      "Early stopping, best iteration is:\n",
      "[7262]\ttraining's auc: 0.941865\tvalid_1's auc: 0.898107\n",
      "Fold 1 started at Fri Mar  1 01:08:31 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[300]\ttraining's auc: 0.827751\tvalid_1's auc: 0.814418\n",
      "[600]\ttraining's auc: 0.862694\tvalid_1's auc: 0.846376\n",
      "[900]\ttraining's auc: 0.880661\tvalid_1's auc: 0.861617\n",
      "[1200]\ttraining's auc: 0.89197\tvalid_1's auc: 0.870773\n",
      "[1500]\ttraining's auc: 0.899986\tvalid_1's auc: 0.876916\n",
      "[1800]\ttraining's auc: 0.906034\tvalid_1's auc: 0.881229\n",
      "[2100]\ttraining's auc: 0.91104\tvalid_1's auc: 0.884702\n",
      "[2400]\ttraining's auc: 0.915125\tvalid_1's auc: 0.88748\n",
      "[2700]\ttraining's auc: 0.918422\tvalid_1's auc: 0.889738\n",
      "[3000]\ttraining's auc: 0.921223\tvalid_1's auc: 0.891263\n",
      "[3300]\ttraining's auc: 0.923545\tvalid_1's auc: 0.892692\n",
      "[3600]\ttraining's auc: 0.925664\tvalid_1's auc: 0.8939\n",
      "[3900]\ttraining's auc: 0.927489\tvalid_1's auc: 0.894689\n",
      "[4200]\ttraining's auc: 0.929129\tvalid_1's auc: 0.895636\n",
      "[4500]\ttraining's auc: 0.930505\tvalid_1's auc: 0.896254\n",
      "[4800]\ttraining's auc: 0.931932\tvalid_1's auc: 0.896753\n",
      "[5100]\ttraining's auc: 0.933245\tvalid_1's auc: 0.897245\n",
      "[5400]\ttraining's auc: 0.934434\tvalid_1's auc: 0.897676\n",
      "[5700]\ttraining's auc: 0.935576\tvalid_1's auc: 0.898036\n",
      "[6000]\ttraining's auc: 0.93674\tvalid_1's auc: 0.898167\n",
      "[6300]\ttraining's auc: 0.937896\tvalid_1's auc: 0.898315\n",
      "[6600]\ttraining's auc: 0.939002\tvalid_1's auc: 0.898441\n",
      "[6900]\ttraining's auc: 0.940133\tvalid_1's auc: 0.898543\n",
      "[7200]\ttraining's auc: 0.941265\tvalid_1's auc: 0.898681\n",
      "Early stopping, best iteration is:\n",
      "[7270]\ttraining's auc: 0.941529\tvalid_1's auc: 0.898721\n",
      "Fold 2 started at Fri Mar  1 01:15:34 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[300]\ttraining's auc: 0.827051\tvalid_1's auc: 0.814447\n",
      "[600]\ttraining's auc: 0.862015\tvalid_1's auc: 0.846472\n",
      "[900]\ttraining's auc: 0.879556\tvalid_1's auc: 0.862552\n",
      "[1200]\ttraining's auc: 0.890617\tvalid_1's auc: 0.872344\n",
      "[1500]\ttraining's auc: 0.898819\tvalid_1's auc: 0.879328\n",
      "[1800]\ttraining's auc: 0.905134\tvalid_1's auc: 0.88467\n",
      "[2100]\ttraining's auc: 0.909774\tvalid_1's auc: 0.888424\n",
      "[2400]\ttraining's auc: 0.913715\tvalid_1's auc: 0.891313\n",
      "[2700]\ttraining's auc: 0.91699\tvalid_1's auc: 0.893762\n",
      "[3000]\ttraining's auc: 0.919726\tvalid_1's auc: 0.895966\n",
      "[3300]\ttraining's auc: 0.922112\tvalid_1's auc: 0.897568\n",
      "[3600]\ttraining's auc: 0.924303\tvalid_1's auc: 0.898953\n",
      "[3900]\ttraining's auc: 0.926179\tvalid_1's auc: 0.899962\n",
      "[4200]\ttraining's auc: 0.927827\tvalid_1's auc: 0.900843\n",
      "[4500]\ttraining's auc: 0.929295\tvalid_1's auc: 0.90161\n",
      "[4800]\ttraining's auc: 0.930666\tvalid_1's auc: 0.902228\n",
      "[5100]\ttraining's auc: 0.932031\tvalid_1's auc: 0.902531\n",
      "[5400]\ttraining's auc: 0.933261\tvalid_1's auc: 0.902963\n",
      "[5700]\ttraining's auc: 0.934496\tvalid_1's auc: 0.90331\n",
      "[6000]\ttraining's auc: 0.935635\tvalid_1's auc: 0.90353\n",
      "[6300]\ttraining's auc: 0.936773\tvalid_1's auc: 0.903782\n",
      "[6600]\ttraining's auc: 0.937967\tvalid_1's auc: 0.903926\n",
      "[6900]\ttraining's auc: 0.939166\tvalid_1's auc: 0.90405\n",
      "[7200]\ttraining's auc: 0.940283\tvalid_1's auc: 0.904113\n",
      "[7500]\ttraining's auc: 0.941393\tvalid_1's auc: 0.904212\n",
      "[7800]\ttraining's auc: 0.942585\tvalid_1's auc: 0.904267\n",
      "[8100]\ttraining's auc: 0.943663\tvalid_1's auc: 0.9043\n",
      "[8400]\ttraining's auc: 0.94479\tvalid_1's auc: 0.904334\n",
      "Early stopping, best iteration is:\n",
      "[8289]\ttraining's auc: 0.944369\tvalid_1's auc: 0.904366\n",
      "Fold 3 started at Fri Mar  1 01:23:42 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[300]\ttraining's auc: 0.827841\tvalid_1's auc: 0.807053\n",
      "[600]\ttraining's auc: 0.86207\tvalid_1's auc: 0.840497\n",
      "[900]\ttraining's auc: 0.880144\tvalid_1's auc: 0.856355\n",
      "[1200]\ttraining's auc: 0.891374\tvalid_1's auc: 0.866072\n",
      "[1500]\ttraining's auc: 0.900156\tvalid_1's auc: 0.873288\n",
      "[1800]\ttraining's auc: 0.906403\tvalid_1's auc: 0.87834\n",
      "[2100]\ttraining's auc: 0.911156\tvalid_1's auc: 0.881947\n",
      "[2400]\ttraining's auc: 0.915244\tvalid_1's auc: 0.884734\n",
      "[2700]\ttraining's auc: 0.918671\tvalid_1's auc: 0.887051\n",
      "[3000]\ttraining's auc: 0.921501\tvalid_1's auc: 0.888721\n",
      "[3300]\ttraining's auc: 0.92391\tvalid_1's auc: 0.890267\n",
      "[3600]\ttraining's auc: 0.926001\tvalid_1's auc: 0.891602\n",
      "[3900]\ttraining's auc: 0.927806\tvalid_1's auc: 0.892728\n",
      "[4200]\ttraining's auc: 0.929385\tvalid_1's auc: 0.8936\n",
      "[4500]\ttraining's auc: 0.930864\tvalid_1's auc: 0.89434\n",
      "[4800]\ttraining's auc: 0.93224\tvalid_1's auc: 0.894984\n",
      "[5100]\ttraining's auc: 0.933493\tvalid_1's auc: 0.89557\n",
      "[5400]\ttraining's auc: 0.934708\tvalid_1's auc: 0.895962\n",
      "[5700]\ttraining's auc: 0.935878\tvalid_1's auc: 0.896372\n",
      "[6000]\ttraining's auc: 0.937057\tvalid_1's auc: 0.89667\n",
      "[6300]\ttraining's auc: 0.93822\tvalid_1's auc: 0.896848\n",
      "[6600]\ttraining's auc: 0.939343\tvalid_1's auc: 0.896974\n",
      "[6900]\ttraining's auc: 0.940541\tvalid_1's auc: 0.897128\n",
      "[7200]\ttraining's auc: 0.941677\tvalid_1's auc: 0.897262\n",
      "[7500]\ttraining's auc: 0.942832\tvalid_1's auc: 0.897298\n",
      "[7800]\ttraining's auc: 0.94396\tvalid_1's auc: 0.897443\n",
      "[8100]\ttraining's auc: 0.945075\tvalid_1's auc: 0.897549\n",
      "[8400]\ttraining's auc: 0.946161\tvalid_1's auc: 0.897663\n",
      "Early stopping, best iteration is:\n",
      "[8446]\ttraining's auc: 0.946319\tvalid_1's auc: 0.897674\n",
      "Fold 4 started at Fri Mar  1 01:31:48 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[300]\ttraining's auc: 0.829989\tvalid_1's auc: 0.811794\n",
      "[600]\ttraining's auc: 0.864709\tvalid_1's auc: 0.840536\n",
      "[900]\ttraining's auc: 0.882118\tvalid_1's auc: 0.854439\n",
      "[1200]\ttraining's auc: 0.893194\tvalid_1's auc: 0.863714\n",
      "[1500]\ttraining's auc: 0.901151\tvalid_1's auc: 0.869946\n",
      "[1800]\ttraining's auc: 0.906977\tvalid_1's auc: 0.874971\n",
      "[2100]\ttraining's auc: 0.911673\tvalid_1's auc: 0.878799\n",
      "[2400]\ttraining's auc: 0.915598\tvalid_1's auc: 0.881841\n",
      "[2700]\ttraining's auc: 0.918896\tvalid_1's auc: 0.884324\n",
      "[3000]\ttraining's auc: 0.921519\tvalid_1's auc: 0.88601\n",
      "[3300]\ttraining's auc: 0.923945\tvalid_1's auc: 0.887669\n",
      "[3600]\ttraining's auc: 0.926012\tvalid_1's auc: 0.88915\n",
      "[3900]\ttraining's auc: 0.927801\tvalid_1's auc: 0.890259\n",
      "[4200]\ttraining's auc: 0.929471\tvalid_1's auc: 0.89115\n",
      "[4500]\ttraining's auc: 0.930907\tvalid_1's auc: 0.891936\n",
      "[4800]\ttraining's auc: 0.932269\tvalid_1's auc: 0.892578\n",
      "[5100]\ttraining's auc: 0.933583\tvalid_1's auc: 0.893007\n",
      "[5400]\ttraining's auc: 0.934782\tvalid_1's auc: 0.893408\n",
      "[5700]\ttraining's auc: 0.936039\tvalid_1's auc: 0.893808\n",
      "[6000]\ttraining's auc: 0.937277\tvalid_1's auc: 0.894164\n",
      "[6300]\ttraining's auc: 0.938437\tvalid_1's auc: 0.894302\n",
      "[6600]\ttraining's auc: 0.939574\tvalid_1's auc: 0.894535\n",
      "[6900]\ttraining's auc: 0.940683\tvalid_1's auc: 0.894651\n",
      "[7200]\ttraining's auc: 0.94187\tvalid_1's auc: 0.894709\n",
      "[7500]\ttraining's auc: 0.942978\tvalid_1's auc: 0.894746\n",
      "[7800]\ttraining's auc: 0.9441\tvalid_1's auc: 0.894834\n",
      "[8100]\ttraining's auc: 0.945225\tvalid_1's auc: 0.894851\n",
      "Early stopping, best iteration is:\n",
      "[7952]\ttraining's auc: 0.944654\tvalid_1's auc: 0.894875\n"
     ]
    }
   ],
   "source": [
    "prediction = np.zeros(len(X_test))\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        \n",
    "    model = lgb.train(params,train_data,num_boost_round=20000,\n",
    "                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 200)\n",
    "            \n",
    "    #y_pred_valid = model.predict(X_valid)\n",
    "    prediction += model.predict(X_test, num_iteration=model.best_iteration)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08114897, 0.2424331 , 0.21381503, ..., 0.003254  , 0.09354758,\n",
       "       0.07144961])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "sub[\"target\"] = prediction\n",
    "sub.to_csv(\"submission_6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liscense\n",
    "Copyright (c) 2019 Hao Zhou\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations\n",
    "Exploratory Data Analysis: https://www.kaggle.com/gpreda/santander-eda-and-prediction\n",
    "\n",
    "LightGBM : https://www.kaggle.com/deepak525/sctp-lightgbm-lb-0-899\n",
    "\n",
    "XGBoost Classifier : https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn\n",
    "\n",
    "XGBoost Regression : https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "Hyper parameter tuning: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "\n",
    "H2O AutoML: https://github.com/nikbearbrown/Kaggle/blob/master/NBB_Zillow_House_Price_Prediction_Feature_Selection.ipynb\n",
    "\n",
    "# Conclusion\n",
    "We used the Santander_Transaction_Customer_Prediction dataset to predict the target by initially running the base model on H2O AutoML and got ensembled stack learning as the best model with an accuracy of 87.23%. As the first part of the assignment we got a rank of 1948 out of 3000 in the Kaggle Competition which was not a good score and then we started to train the model using LightGBM and predicted the target.We ran 3 models which were LightGBM, catboost and XGboost. Lightgbm predicted the best accuracy out of all the 3 models. The final step was tunning the hyperparameters of the model by adding more number of leaves, maxbin,reducing the learning rate,max_depth and num_leaves.We got an accuracy of 89.95% which was better than previous model accuracy.We got even better outcome of 589 out of 3000 in the Kaggle Cometition leaderboard after tunning the hyperparameters.Finally, we conclude that out of all the models, lightgbm performed well which gave the accuracy of 89.48% and after hyperparameter tuning the accuracy changed to 89.95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:left\"> \n",
    "    <thead> \n",
    "    <tr> <th>Number</th> <th>Model Name</th> <th>Accuray</th> </tr> </thead> <tbody> \n",
    "    <tr> \n",
    "         <th scope='row'>1</th> <td style=\"text-align:center\">H2O AutoML</td> <td>0.8723</td> </tr> \n",
    "        <tr> <th scope='row'>2</th> <td style=\"text-align:center\">XGBoosting</td>  <td>0.8921</td> </tr>\n",
    "        <tr> <th scope='row'>3</th> <td style=\"text-align:center\">CatBoosting</td>  <td>0.8910</td> </tr>\n",
    "    <tr> <th scope='row'>4</th> <td style=\"text-align:center\">LightGBM</td>  <td>0.8948</td> </tr>\n",
    "            <tr> <th scope='row'>5</th> <td style=\"text-align:center\">Hyperparameter Tuning</td>  <td>0.8995</td> </tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribution\n",
    "1)The Santander_Transaction_Customer_Prediction dataset which we used we had to predict the probability to identify which products and services might help them achieve their monetary goals.\n",
    "\n",
    "2)The first task was performing H2O AutoML which gave the best model for our predction. We referred Professor Nik Brown's notebook for that and we reduced the features from 201 to 44. Along with that, we also ranked the importance of the variables and deleted some variables according to the threshold and then used that dataset to be trained by H2O.\n",
    "\n",
    "3)We extended the model by hyperparameter tunning using GridSearch CV to improve the model performance.In this we tunned the hyperparameters of the model by adding more number of leaves, maxbin,reducing the learning rate,max_depth and num_leaves.\n",
    "\n",
    "4)We made use of Boosting ensembled learning methods and we used an additional boosting which was lightGBM and Catboosting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
